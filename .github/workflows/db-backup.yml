name: PostgreSQL Backup (Neon â†’ R2)
on:
  schedule:
    - cron: "0 2 * * *"
  workflow_dispatch:

jobs:
  backup:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    env:
      DATABASE_URL: ${{ secrets.DATABASE_URL }}
      R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
      R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
      R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
      R2_BUCKET: ${{ secrets.R2_BUCKET }}
      BACKUP_PREFIX: neon
      
    steps:
      - name: Install PostgreSQL client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client
          
      - name: Check AWS CLI
        run: aws --version
        
      - name: Configure AWS for R2
        run: |
          aws configure set aws_access_key_id "$R2_ACCESS_KEY_ID"
          aws configure set aws_secret_access_key "$R2_SECRET_ACCESS_KEY"
          aws configure set region auto
          
      - name: Test R2 connection
        run: |
          echo "Testing R2 connection..."
          aws s3 ls "s3://$R2_BUCKET" \
            --endpoint-url "https://$R2_ACCOUNT_ID.r2.cloudflarestorage.com" || \
            echo "Warning: Could not list bucket contents"
          
      - name: Create and upload backup
        run: |
          set -e
          TIMESTAMP=$(date +"%Y-%m-%d_%H-%M")
          FILE="${BACKUP_PREFIX}_${TIMESTAMP}.dump"
          
          echo "=== Starting backup process ==="
          echo "Timestamp: $TIMESTAMP"
          
          echo -e "\n=== Dumping database ==="
          pg_dump "$DATABASE_URL" \
            --format=custom \
            --no-owner \
            --no-privileges \
            --verbose \
            > "$FILE" 2>&1 | tail -20
          
          DUMP_SIZE=$(stat -f%z "$FILE" 2>/dev/null || stat -c%s "$FILE")
          echo "Dump size: $(numfmt --to=iec-i --suffix=B $DUMP_SIZE)"
          
          echo -e "\n=== Compressing backup ==="
          gzip -v "$FILE"
          
          COMPRESSED_SIZE=$(stat -f%z "$FILE.gz" 2>/dev/null || stat -c%s "$FILE.gz")
          echo "Compressed size: $(numfmt --to=iec-i --suffix=B $COMPRESSED_SIZE)"
          
          RATIO=$((DUMP_SIZE / COMPRESSED_SIZE))
          echo "Compression ratio: ${RATIO}:1"
          
          echo -e "\n=== Uploading to R2 ==="
          aws s3 cp "$FILE.gz" \
            "s3://$R2_BUCKET/$FILE.gz" \
            --endpoint-url "https://$R2_ACCOUNT_ID.r2.cloudflarestorage.com"
          
          echo -e "\n=== Backup completed successfully ==="
          echo "File: $FILE.gz"
          echo "Size: $(numfmt --to=iec-i --suffix=B $COMPRESSED_SIZE)"
          
      - name: Cleanup old backups (keep last 7 days)
        run: |
          echo "Cleaning up old backups..."
          CUTOFF_DATE=$(date -d '7 days ago' +%Y-%m-%d)
          
          aws s3 ls "s3://$R2_BUCKET/" \
            --endpoint-url "https://$R2_ACCOUNT_ID.r2.cloudflarestorage.com" | \
          while read -r line; do
            FILE_DATE=$(echo "$line" | awk '{print $1}')
            FILE_NAME=$(echo "$line" | awk '{print $4}')
            
            if [[ "$FILE_NAME" == ${BACKUP_PREFIX}_* ]] && [[ "$FILE_DATE" < "$CUTOFF_DATE" ]]; then
              echo "Deleting old backup: $FILE_NAME"
              aws s3 rm "s3://$R2_BUCKET/$FILE_NAME" \
                --endpoint-url "https://$R2_ACCOUNT_ID.r2.cloudflarestorage.com"
            fi
          done || echo "No old backups to clean"
